import json
import random
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import numpy as np

def generate_urdu_keywords():
    """Generate Urdu keywords that were popular during 9th May 2023"""
    
    urdu_keywords = {
        # Political terms
        "Ø¹Ù…Ø±Ø§Ù† Ø®Ø§Ù†": {"sentiment": "positive", "frequency": 12500},
        "Ù¾ÛŒ Ù¹ÛŒ Ø¢Ø¦ÛŒ": {"sentiment": "positive", "frequency": 8900},
        "Ù†Ùˆ Ù…Ø¦ÛŒ": {"sentiment": "negative", "frequency": 15200},
        "Ú¯Ø±ÙØªØ§Ø±ÛŒ": {"sentiment": "negative", "frequency": 7800},
        "Ø§Ù†ØµØ§Ù": {"sentiment": "positive", "frequency": 5200},
        
        # Violence related
        "ØªØ´Ø¯Ø¯": {"sentiment": "negative", "frequency": 9800},
        "ØªÙˆÚ‘ Ù¾Ú¾ÙˆÚ‘": {"sentiment": "negative", "frequency": 4500},
        "Ø­Ù…Ù„Û": {"sentiment": "negative", "frequency": 6200},
        "Ø§ÙØ±Ø§ØªÙØ±ÛŒ": {"sentiment": "negative", "frequency": 3800},
        "Ø§Ù†ØªÛØ§ Ù¾Ø³Ù†Ø¯ÛŒ": {"sentiment": "negative", "frequency": 4200},
        
        # Locations
        "Ù„Ø§ÛÙˆØ±": {"sentiment": "neutral", "frequency": 6800},
        "Ø§Ø³Ù„Ø§Ù… Ø¢Ø¨Ø§Ø¯": {"sentiment": "neutral", "frequency": 5500},
        "Ú©Ø±Ø§Ú†ÛŒ": {"sentiment": "neutral", "frequency": 4200},
        "Ù¾Ø§Ú©Ø³ØªØ§Ù†": {"sentiment": "neutral", "frequency": 8900},
        
        # Support/Opposition
        "Ø­Ø§Ù…ÛŒ": {"sentiment": "positive", "frequency": 3500},
        "Ù…Ø®Ø§Ù„Ù": {"sentiment": "negative", "frequency": 2800},
        "Ø§Ø­ØªØ¬Ø§Ø¬": {"sentiment": "neutral", "frequency": 5800},
        "Ù…Ø¸Ø§ÛØ±Û": {"sentiment": "neutral", "frequency": 4200},
        
        # Government/Military
        "ÙÙˆØ¬": {"sentiment": "negative", "frequency": 4800},
        "Ø­Ú©ÙˆÙ…Øª": {"sentiment": "negative", "frequency": 5200},
        "Ø¹Ø¯Ø§Ù„Øª": {"sentiment": "neutral", "frequency": 3800},
        "Ù¾ÙˆÙ„ÛŒØ³": {"sentiment": "negative", "frequency": 4500},
        
        # Emotions/Reactions
        "ØºØµÛ": {"sentiment": "negative", "frequency": 3200},
        "Ø®ÙˆØ´ÛŒ": {"sentiment": "positive", "frequency": 2100},
        "ØºÙ…": {"sentiment": "negative", "frequency": 2800},
        "Ø§Ù…ÛŒØ¯": {"sentiment": "positive", "frequency": 2500},
        "ÚˆØ±": {"sentiment": "negative", "frequency": 3500},
        
        # Democratic terms
        "Ø¬Ù…ÛÙˆØ±ÛŒØª": {"sentiment": "positive", "frequency": 3800},
        "Ø¢Ø²Ø§Ø¯ÛŒ": {"sentiment": "positive", "frequency": 3200},
        "Ø­Ù‚ÙˆÙ‚": {"sentiment": "positive", "frequency": 2800},
        "Ù‚Ø§Ù†ÙˆÙ†": {"sentiment": "neutral", "frequency": 3500},
    }
    
    return urdu_keywords

def analyze_urdu_sentiment_patterns():
    """Analyze sentiment patterns in Urdu keywords"""
    
    keywords = generate_urdu_keywords()
    
    print("=== Ø§Ø±Ø¯Ùˆ Ú©Ù„ÛŒØ¯ÛŒ Ø§Ù„ÙØ§Ø¸ Ú©Ø§ Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ - Û¹ Ù…Ø¦ÛŒ Û²Û°Û²Û³ ===")
    print("Urdu Keywords Sentiment Analysis - 9th May 2023")
    print("=" * 60)
    
    # Categorize by sentiment
    positive_words = []
    negative_words = []
    neutral_words = []
    
    for word, data in keywords.items():
        if data["sentiment"] == "positive":
            positive_words.append((word, data["frequency"]))
        elif data["sentiment"] == "negative":
            negative_words.append((word, data["frequency"]))
        else:
            neutral_words.append((word, data["frequency"]))
    
    # Sort by frequency
    positive_words.sort(key=lambda x: x[1], reverse=True)
    negative_words.sort(key=lambda x: x[1], reverse=True)
    neutral_words.sort(key=lambda x: x[1], reverse=True)
    
    print("\nğŸŸ¢ Ù…Ø«Ø¨Øª Ø¬Ø°Ø¨Ø§Øª (Positive Sentiment):")
    for word, freq in positive_words[:5]:
        print(f"   {word}: {freq:,} mentions")
    
    print("\nğŸ”´ Ù…Ù†ÙÛŒ Ø¬Ø°Ø¨Ø§Øª (Negative Sentiment):")
    for word, freq in negative_words[:5]:
        print(f"   {word}: {freq:,} mentions")
    
    print("\nâšª ØºÛŒØ± Ø¬Ø§Ù†Ø¨Ø¯Ø§Ø± (Neutral Sentiment):")
    for word, freq in neutral_words[:5]:
        print(f"   {word}: {freq:,} mentions")
    
    # Calculate overall statistics
    total_positive = sum(data["frequency"] for data in keywords.values() if data["sentiment"] == "positive")
    total_negative = sum(data["frequency"] for data in keywords.values() if data["sentiment"] == "negative")
    total_neutral = sum(data["frequency"] for data in keywords.values() if data["sentiment"] == "neutral")
    total_all = total_positive + total_negative + total_neutral
    
    print(f"\nğŸ“Š Ù…Ø¬Ù…ÙˆØ¹ÛŒ Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ø´Ù…Ø§Ø± (Overall Statistics):")
    print(f"   Ù…Ø«Ø¨Øª Ø§Ù„ÙØ§Ø¸ (Positive): {total_positive:,} ({total_positive/total_all*100:.1f}%)")
    print(f"   Ù…Ù†ÙÛŒ Ø§Ù„ÙØ§Ø¸ (Negative): {total_negative:,} ({total_negative/total_all*100:.1f}%)")
    print(f"   ØºÛŒØ± Ø¬Ø§Ù†Ø¨Ø¯Ø§Ø± Ø§Ù„ÙØ§Ø¸ (Neutral): {total_neutral:,} ({total_neutral/total_all*100:.1f}%)")
    
    return keywords

def generate_hashtag_combinations():
    """Generate popular hashtag combinations used during the incident"""
    
    hashtag_combinations = [
        "#Ø¹Ù…Ø±Ø§Ù†_Ø®Ø§Ù† OR #ImranKhan",
        "#Ù†Ùˆ_Ù…Ø¦ÛŒ OR #9thMay", 
        "#Ù¾ÛŒ_Ù¹ÛŒ_Ø¢Ø¦ÛŒ OR #PTI",
        "#ØªØ´Ø¯Ø¯ OR #Violence",
        "#Ø§Ù†ØµØ§Ù OR #Justice",
        "#Ù¾Ø§Ú©Ø³ØªØ§Ù† OR #Pakistan",
        "#Ù„Ø§ÛÙˆØ± OR #Lahore",
        "#Ú¯Ø±ÙØªØ§Ø±ÛŒ OR #Arrest",
        "#Ø§Ø­ØªØ¬Ø§Ø¬ OR #Protests",
        "#Ø¬Ù…ÛÙˆØ±ÛŒØª OR #Democracy"
    ]
    
    print("\nğŸ” Ù…Ù‚Ø¨ÙˆÙ„ ÛÛŒØ´ Ù¹ÛŒÚ¯ Ø§Ù…ØªØ²Ø§Ø¬ (Popular Hashtag Combinations):")
    for i, combo in enumerate(hashtag_combinations, 1):
        print(f"   {i}. {combo}")
    
    return hashtag_combinations

def simulate_hourly_urdu_trends():
    """Simulate hourly trends for Urdu keywords on May 9th"""
    
    hours = ["ØµØ¨Ø­ Û¶", "ØµØ¨Ø­ Û¸", "ØµØ¨Ø­ Û±Û°", "Ø¯ÙˆÙ¾ÛØ± Û±Û²", "Ø¯ÙˆÙ¾ÛØ± Û²", "Ø´Ø§Ù… Û´", "Ø´Ø§Ù… Û¶", "Ø±Ø§Øª Û¸", "Ø±Ø§Øª Û±Û°"]
    english_hours = ["6 AM", "8 AM", "10 AM", "12 PM", "2 PM", "4 PM", "6 PM", "8 PM", "10 PM"]
    
    # Key Urdu terms with hourly variations
    urdu_trends = {
        "Ø¹Ù…Ø±Ø§Ù† Ø®Ø§Ù†": [450, 680, 890, 1200, 1800, 1500, 1200, 950, 720],
        "Ù†Ùˆ Ù…Ø¦ÛŒ": [200, 450, 780, 1500, 2100, 1800, 1400, 1100, 850],
        "ØªØ´Ø¯Ø¯": [100, 200, 400, 800, 1200, 1000, 700, 500, 300],
        "Ø§Ù†ØµØ§Ù": [300, 400, 500, 600, 800, 750, 650, 550, 450],
        "Ù¾ÛŒ Ù¹ÛŒ Ø¢Ø¦ÛŒ": [250, 350, 450, 650, 900, 800, 650, 500, 400]
    }
    
    print(f"\nâ° Ú¯Ú¾Ù†Ù¹Û ÙˆØ§Ø± Ø±Ø¬Ø­Ø§Ù†Ø§Øª - Û¹ Ù…Ø¦ÛŒ (Hourly Trends - May 9th):")
    print("=" * 50)
    
    for hour_urdu, hour_eng in zip(hours, english_hours):
        print(f"\n{hour_urdu} ({hour_eng}):")
        hour_index = english_hours.index(hour_eng)
        
        for term, values in urdu_trends.items():
            print(f"   {term}: {values[hour_index]:,} mentions")
    
    return urdu_trends

def create_search_query_examples():
    """Create example search queries mixing English and Urdu"""
    
    search_examples = [
        {
            "query": "#ImranKhan OR #Ø¹Ù…Ø±Ø§Ù†_Ø®Ø§Ù† OR #PTI OR #Ù¾ÛŒ_Ù¹ÛŒ_Ø¢Ø¦ÛŒ",
            "description": "Pro-Imran Khan sentiment tracking",
            "date_range": "2023-05-07 to 2023-05-15"
        },
        {
            "query": "#9thMay OR #Ù†Ùˆ_Ù…Ø¦ÛŒ OR #Violence OR #ØªØ´Ø¯Ø¯",
            "description": "Violence and incident tracking", 
            "date_range": "2023-05-09 to 2023-05-12"
        },
        {
            "query": "#Lahore OR #Ù„Ø§ÛÙˆØ± OR #Islamabad OR #Ø§Ø³Ù„Ø§Ù…_Ø¢Ø¨Ø§Ø¯",
            "description": "City-specific sentiment analysis",
            "date_range": "2023-05-09 to 2023-05-10"
        },
        {
            "query": "#Justice OR #Ø§Ù†ØµØ§Ù OR #Democracy OR #Ø¬Ù…ÛÙˆØ±ÛŒØª",
            "description": "Democratic values sentiment",
            "date_range": "2023-05-07 to 2023-05-20"
        },
        {
            "query": "#Arrest OR #Ú¯Ø±ÙØªØ§Ø±ÛŒ OR #Court OR #Ø¹Ø¯Ø§Ù„Øª",
            "description": "Legal proceedings tracking",
            "date_range": "2023-05-09 to 2023-05-15"
        }
    ]
    
    print(f"\nğŸ” ØªØ¬ÙˆÛŒØ² Ú©Ø±Ø¯Û ØªÙ„Ø§Ø´ Ú©ÛŒ Ù…Ø«Ø§Ù„ÛŒÚº (Suggested Search Examples):")
    print("=" * 60)
    
    for i, example in enumerate(search_examples, 1):
        print(f"\n{i}. {example['description']}")
        print(f"   Query: {example['query']}")
        print(f"   Date Range: {example['date_range']}")
    
    return search_examples

def main():
    print("ğŸ‡µğŸ‡° Pakistan Social Media Sentiment Analysis - Urdu Keywords")
    print("=" * 70)
    
    # Analyze Urdu keywords
    keywords = analyze_urdu_sentiment_patterns()
    
    # Generate hashtag combinations
    hashtags = generate_hashtag_combinations()
    
    # Show hourly trends
    trends = simulate_hourly_urdu_trends()
    
    # Create search examples
    examples = create_search_query_examples()
    
    print(f"\nğŸ“ˆ Ø®Ù„Ø§ØµÛ (Summary):")
    print("=" * 30)
    print("âœ… Urdu keywords analysis completed")
    print("âœ… Hashtag combinations generated") 
    print("âœ… Hourly trends calculated")
    print("âœ… Search query examples created")
    print("\nğŸ¯ Use these insights to track sentiment more effectively!")

if __name__ == "__main__":
    main()
